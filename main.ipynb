{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7ada03-450a-44a4-a68e-eccd19a33378",
   "metadata": {},
   "source": [
    "# Birdsong Classification #\n",
    "\n",
    "This project is an attempt to determine the species of a bird given a short recording of it's birdsong. The data was obtained here: https://www.kaggle.com/datasets/vinayshanbhag/bird-song-data-set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac29e30-c5ad-4545-b28a-a99d5444731a",
   "metadata": {},
   "source": [
    "The data contains over 5000 3-second recordings of birdsongs in wav format, and each of which belongs to one of five species:\n",
    "\n",
    "* Bewick's Wren (Thyromanes Bewickii)\n",
    "* Northern Cardinal (Cardinalis Cardinalis)\n",
    "* American Robin (Turdus Migratorius)\n",
    "* Song Sparrow (Melospiza Melodia)\n",
    "* Northern Mockingbird (Mimus Polyglottos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb4e3130-1f38-4aed-a6cb-cb7a61901fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./features'):\n",
    "    os.mkdir('./features')\n",
    "    \n",
    "if not os.path.exists('./figures'):\n",
    "    os.mkdir('./figures')\n",
    "\n",
    "if not os.path.exists('./parameters'):\n",
    "    os.mkdir('./parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6c9c29-09d5-4d83-a2cf-3202240838b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id       genus     species      filename\n",
      "0     557838  Thryomanes    bewickii  557838-0.wav\n",
      "1     557838  Thryomanes    bewickii  557838-1.wav\n",
      "2     557838  Thryomanes    bewickii  557838-4.wav\n",
      "3     557838  Thryomanes    bewickii  557838-5.wav\n",
      "4     557838  Thryomanes    bewickii  557838-6.wav\n",
      "...      ...         ...         ...           ...\n",
      "5417   11713  Cardinalis  cardinalis   11713-8.wav\n",
      "5418   11713  Cardinalis  cardinalis  11713-10.wav\n",
      "5419   11713  Cardinalis  cardinalis  11713-14.wav\n",
      "5420   11713  Cardinalis  cardinalis  11713-15.wav\n",
      "5421   11713  Cardinalis  cardinalis  11713-16.wav\n",
      "\n",
      "[5422 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./archive/bird_songs_metadata.csv')[['id','genus','species','filename']]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf4092-faac-4117-b327-3a984e47fd9b",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "\n",
    "### 1) Mel-Frequency Cepstrum Coefficients\n",
    "Many feature engineernig methods for audio files exist. One of the most common is the **Mel-Frequency Cepstrum Coefficients (MFCC)**. We will begin with this, but also experiment with others.\n",
    "\n",
    "There is significant freedom in specifying the window size and window shift length in the spectrogram. It will be important to balance information loss with high dimensionality in this feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b149569-8331-43d9-ae05-b8ec1ae7b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "sample_rate = 22050 # sample rate given from dataset description\n",
    "win_length_seconds = 0.04 # specify 40ms window sizes\n",
    "\n",
    "win_length = int(sample_rate*win_length_seconds) \n",
    "hop_length = int(sample_rate*win_length_seconds/2) # 1/2 window shift\n",
    "\n",
    "n_mfcc = 30 # number of desired coefficients per window\n",
    "n_windows = sample_rate*3//hop_length -1 # total number of windows\n",
    "\n",
    "# store coefficients in array\n",
    "coefficients = np.zeros([len(data),n_mfcc*n_windows])\n",
    "\n",
    "# specify MFCC transform\n",
    "transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate = 22050,\n",
    "    n_mfcc = n_mfcc,\n",
    "    melkwargs={'n_fft':win_length, 'n_mels': 70, 'center': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731263b2-6d54-4d0a-b7db-c7da06a566d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through audio files, produce MFCCs for each signal\n",
    "for i,filename in enumerate(data['filename']):\n",
    "    waveform,_ = torchaudio.load('./archive/wavfiles/'+filename)\n",
    "    coefficients[i] = transform(waveform).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217d5725-4c3a-4842-9701-b6f057416ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store features with data for later use\n",
    "mfcc_dataframe = pd.concat([data[['species']],pd.DataFrame(coefficients)],axis=1)\n",
    "mfcc_dataframe.to_csv('./features/mfcc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd127e82-8bd4-4879-8aa9-431f3c82293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5422, 4470)\n"
     ]
    }
   ],
   "source": [
    "print(coefficients.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da204cee-bd2b-4f40-ad51-17bfae05a253",
   "metadata": {},
   "source": [
    "The dimension of the data is quite large. We will use Principal Component Analysis to reduce the size. In the future, it may be interesting to investigate whether it is better to reduce the dimension of the dataset by using a larger window size for the spectrum, or choose small window sizes and reduce the dimension afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b6d5c2-0277-4cbc-bcf3-228522e7d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 100 # begin with 100 components\n",
    "pca = PCA(n_components = n_components)\n",
    "pca.fit(coefficients)\n",
    "mfcc_pca = pca.transform(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c7c568-ddd0-43bb-8a52-c66d5a9425d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_pca_dataframe = pd.concat([data[['species']],pd.DataFrame(mfcc_pca)],axis=1)\n",
    "mfcc_dataframe.to_csv('./features/mfcc_pca.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6d90f-08e7-482c-a321-14cefa639657",
   "metadata": {},
   "source": [
    "### 2) Chroma Features\n",
    "\n",
    "This method extracts information from a waveform by mapping its spectrum to twelve bins representing the chromatic scale. The intuition behind this method is that it captures the harmonic characteristics of the waveform; signals of similar pitch will have similar chroma features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "995c0224-5f85-4c48-bb26-58f6e5e4f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminfrizzell/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "chroma_features = np.zeros([len(data),12*151])\n",
    "\n",
    "for i,filename in enumerate(data['filename']):\n",
    "    waveform,_ = librosa.load('./archive/wavfiles/'+filename)\n",
    "    \n",
    "    chromagram = librosa.feature.chroma_stft(\n",
    "    y = waveform,\n",
    "    n_fft = win_length,\n",
    "    hop_length = hop_length,\n",
    "    sr = sample_rate\n",
    "    )\n",
    "    \n",
    "    chroma_features[i] =  chromagram.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a148186-7605-4335-a417-9789fd3c62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(chroma_features)\n",
    "chroma_features_pca = pca.transform(chroma_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "72fe36c6-ff66-45a8-9f04-ef6a31f2e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_feature_dataframe = pd.concat([data[['species']],pd.DataFrame(chroma_features)],axis=1)\n",
    "chroma_feature_dataframe.to_csv('./features/chroma_features.csv')\n",
    "\n",
    "chroma_feature_pca_dataframe = pd.concat([data[['species']],pd.DataFrame(chroma_features_pca)],axis=1)\n",
    "chroma_feature_pca_dataframe.to_csv('./features/chroma_features_pca.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d119baf-073a-4af3-8fc4-5b1b3b3bde2f",
   "metadata": {},
   "source": [
    "### 3) Zero-Crossing Rate\n",
    "\n",
    "We have two feature sets extracted from the frequency domain of the signals, so the final feature set we will consider will be the zero-crossing rate, which is obtained from the time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72d00497-638a-4693-b3a8-3a75790a80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zero_crossing_rates = np.zeros([len(data),151])\n",
    "\n",
    "for i,filename in enumerate(data['filename']):\n",
    "    waveform,_ = librosa.load('./archive/wavfiles/'+filename)\n",
    "    \n",
    "    zero_crossing_rates[i] = librosa.feature.zero_crossing_rate(\n",
    "    y = waveform,\n",
    "    frame_length = win_length,\n",
    "    hop_length = hop_length,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6f79991-77f5-443f-af34-21cdef8d4b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5422, 151)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_crossing_rates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86504166-95a3-4876-818a-cf8a48ede612",
   "metadata": {},
   "source": [
    "The dimensionality of this feature set is reasonably small, so we will not concern ourselves with dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "774313b9-dc0d-4369-a0bd-38ae3e797bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_data = pd.concat([data[['species']],pd.DataFrame(zero_crossing_rates)],axis=1)\n",
    "zcr_data.to_csv('./features/zero_crossing_rates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06561816-6613-425e-bf9b-9a9af2fee518",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "\n",
    "We now have two feature sets with which to test and evaluate models. We will begin with some common classifier models:\n",
    "\n",
    "* Random Forests\n",
    "* Support Vector Machines\n",
    "* Multilayer Perceptron\n",
    "* K-Nearest Neighbour\n",
    "* Naive Bayes\n",
    "\n",
    " Hyperparameters will be determined by minimizing validation error using **Bayesian Optimization**, a powerful method used for optimizing multivariate functions that have a consideral computational cost. \n",
    "\n",
    "Read more about the method here: https://arxiv.org/abs/1807.02811.\n",
    "\n",
    "This is the implementation used: https://github.com/bayesian-optimization/BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb94f6-91f3-43c6-8b71-5685819a0e4e",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "We will start with the `RandomForestClassifier` from scikit-learn. Hyperparameters to consider with this model are:\n",
    "* The number of trees in the forest: `n_estimators`\n",
    "* The number of bootstrapped samples per tree: `max_samples`\n",
    "* The cost-complexity pruning parameter: `ccp_alpha` (We will use post-pruning in this analysis, and not consider `max_depth`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "665bf292-52cd-47aa-99d4-5970795320ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SEED = 1\n",
    "mfcc_dataframe = pd.read_csv('./features/mfcc_pca.csv').iloc[:,1:]\n",
    "cf_dataframe = pd.read_csv('./features/chroma_features_pca.csv').iloc[:,1:]\n",
    "zcr_dataframe = pd.read_csv('./features/zero_crossing_rates.csv').iloc[:,1:]\n",
    "\n",
    "feature_names = ['mfcc','cf','zcr']\n",
    "all_feature_sets = {k:v for k,v in zip(feature_names,[mfcc_dataframe,cf_dataframe,zcr_dataframe])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3b1a0-af44-404b-94b2-2df736d301c0",
   "metadata": {},
   "source": [
    "Since we have several feature sets to consider, we will split each corresponding dataframe into stratified training and testing samples, and store in a dictionary of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c3d3523-bdfc-4726-a14e-d8be0d5ddcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into stratified training and testing datasets\n",
    "training_data = {}\n",
    "testing_data = {}\n",
    "\n",
    "for name in feature_names:\n",
    "    feature_set = all_feature_sets[name]\n",
    "    labels = feature_set.iloc[:,0]\n",
    "    training_data[name],testing_data[name] = train_test_split(feature_set,test_size = 0.25,stratify = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5070f7a-e5bd-42cd-aec5-7bffa89599d8",
   "metadata": {},
   "source": [
    "Before tuning hyperparameters, I would like to plot the training and testing error as a function of each hyperparameter. This may give a good idea of 'obvious' hyperparameter value regions over which to tune the models.\n",
    "\n",
    "The error output will be stored several dictionaries, indexed by feature set type, hyperparameter, and error type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "49b7a358-f52c-4377-b952-4fe1f6300adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# dictionary to store scores\n",
    "scores = {i:{j:{k:[] for k in ['train','test']} for j in ['n_estimators','ccp_alpha','max_samples']} for i in feature_names}\n",
    "N_training_samples = len(training_data['mfcc'])\n",
    "\n",
    "n_estimators_list = np.arange(75,625,25)\n",
    "ccp_alpha_list = np.linspace(0,0.02,30)\n",
    "max_samples_list = np.arange(500,N_training_samples+400,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8708bbcc-ecbf-4347-b5fd-616eff17d9db",
   "metadata": {},
   "source": [
    "We will now train the model over each feature set, and vary each hyperparameter independently while keeping the rest at their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a038c8-d3e0-4155-a9ab-38abf0a7f564",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m feature_names:\n\u001b[1;32m      3\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m training_data[name]\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      4\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m training_data[name]\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "    for name in feature_names:\n",
    "    \n",
    "        X_train = training_data[name].iloc[:,1:]\n",
    "        y_train = training_data[name].iloc[:,0]\n",
    "        \n",
    "        X_test = testing_data[name].iloc[:,1:]\n",
    "        y_test = testing_data[name].iloc[:,0]\n",
    "\n",
    "    # train over varying number of trees\n",
    "        for n_estimators in n_estimators_list:\n",
    "\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators = n_estimators\n",
    "            )\n",
    "            model.fit(X_train,y_train)\n",
    "            train_score = model.score(X_train,y_train)\n",
    "            test_score = model.score(X_test,y_test)\n",
    "            scores['n_estimators'][name]['train'].append(train_score)\n",
    "            scores['n_estimators'][name]['test'].append(test_score)\n",
    "\n",
    "    # train over varying post-pruning parameter\n",
    "        for ccp_alpha in ccp_alpha_list:\n",
    "        \n",
    "            model = RandomForestClassifier(\n",
    "                ccp_alpha = ccp_alpha\n",
    "            )\n",
    "            model.fit(X_train,y_train)\n",
    "            train_score = model.score(X_train,y_train)\n",
    "            test_score = model.score(X_test,y_test)\n",
    "            scores['ccp_alpha'][name]['train'].append(train_score)\n",
    "            scores['ccp_alpha'][name]['test'].append(test_score)\n",
    "\n",
    "    # finally, train over varying number of bootstrapped samples\n",
    "        for max_samples in max_samples_list:\n",
    "        \n",
    "            model = RandomForestClassifier(\n",
    "                max_samples = max_samples\n",
    "            )\n",
    "            model.fit(X_train,y_train)\n",
    "            train_score = model.score(X_train,y_train)\n",
    "            test_score = model.score(X_test,y_test)\n",
    "            scores['max_samples'][name]['train'].append(train_score)\n",
    "            scores['max_samples'][name]['test'].append(test_score)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479773a6-1161-4992-927d-7bdfabcdea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# creating a wrapper for hyperparameter tuning with Bayesian Optimization\n",
    "def optimize_RF(n_iter,init_points,pbounds,X,y):\n",
    "    \n",
    "    def cross_val_wrapper(n_estimators,max_samples,ccp_alpha):\n",
    "    \n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators = int(n_estimators),\n",
    "            max_samples = int(max_samples),\n",
    "            ccp_alpha = ccp_alpha\n",
    "        )\n",
    "    \n",
    "        score = cross_val_score(model,X,y)\n",
    "        return score.mean()\n",
    "\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f = cross_val_wrapper,\n",
    "        pbounds = pbounds,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer.maximize(\n",
    "        n_iter = n_iter,\n",
    "        init_points = init_points\n",
    "    )\n",
    "\n",
    "    optimizer.max['params']['max_samples'] = int(optimizer.max['params']['max_samples'])\n",
    "    optimizer.max['params']['n_estimators'] = int(optimizer.max['params']['n_estimators'])\n",
    "    return optimizer.max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ff2da58-4c80-42c8-bbbe-15f034d17c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | ccp_alpha | max_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.3467   \u001b[0m | \u001b[0m0.02085  \u001b[0m | \u001b[0m2.715e+03\u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.446    \u001b[0m | \u001b[95m0.01512  \u001b[0m | \u001b[95m871.7    \u001b[0m | \u001b[95m136.9    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.4889   \u001b[0m | \u001b[95m0.009313 \u001b[0m | \u001b[95m1.511e+03\u001b[0m | \u001b[95m258.7    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.3423   \u001b[0m | \u001b[0m0.02694  \u001b[0m | \u001b[0m1.747e+03\u001b[0m | \u001b[0m374.1    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.4572   \u001b[0m | \u001b[0m0.01022  \u001b[0m | \u001b[0m3.222e+03\u001b[0m | \u001b[0m111.0    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.2316   \u001b[0m | \u001b[0m0.04654  \u001b[0m | \u001b[0m3.223e+03\u001b[0m | \u001b[0m109.1    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.3403   \u001b[0m | \u001b[0m0.02661  \u001b[0m | \u001b[0m2.931e+03\u001b[0m | \u001b[0m170.0    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.3421   \u001b[0m | \u001b[0m0.02979  \u001b[0m | \u001b[0m1.239e+03\u001b[0m | \u001b[0m398.7    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.4869   \u001b[0m | \u001b[0m0.01138  \u001b[0m | \u001b[0m900.1    \u001b[0m | \u001b[0m478.9    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.337    \u001b[0m | \u001b[0m0.03631  \u001b[0m | \u001b[0m2.742e+03\u001b[0m | \u001b[0m177.9    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.3462   \u001b[0m | \u001b[0m0.02351  \u001b[0m | \u001b[0m1.761e+03\u001b[0m | \u001b[0m288.8    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.2316   \u001b[0m | \u001b[0m0.04777  \u001b[0m | \u001b[0m2.534e+03\u001b[0m | \u001b[0m288.3    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.3412   \u001b[0m | \u001b[0m0.02444  \u001b[0m | \u001b[0m3.104e+03\u001b[0m | \u001b[0m491.6    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.3534   \u001b[0m | \u001b[0m0.01612  \u001b[0m | \u001b[0m2.854e+03\u001b[0m | \u001b[0m442.8    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.28     \u001b[0m | \u001b[0m0.04275  \u001b[0m | \u001b[0m2.712e+03\u001b[0m | \u001b[0m464.8    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.339    \u001b[0m | \u001b[0m0.03386  \u001b[0m | \u001b[0m2.161e+03\u001b[0m | \u001b[0m253.3    \u001b[0m |\n",
      "| \u001b[95m17       \u001b[0m | \u001b[95m0.5915   \u001b[0m | \u001b[95m0.007038 \u001b[0m | \u001b[95m848.0    \u001b[0m | \u001b[95m226.7    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.3938   \u001b[0m | \u001b[0m0.0137   \u001b[0m | \u001b[0m2.857e+03\u001b[0m | \u001b[0m126.7    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.2316   \u001b[0m | \u001b[0m0.04697  \u001b[0m | \u001b[0m3.453e+03\u001b[0m | \u001b[0m211.7    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.344    \u001b[0m | \u001b[0m0.02991  \u001b[0m | \u001b[0m762.2    \u001b[0m | \u001b[0m146.7    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.341    \u001b[0m | \u001b[0m0.03068  \u001b[0m | \u001b[0m2.191e+03\u001b[0m | \u001b[0m361.4    \u001b[0m |\n",
      "| \u001b[95m22       \u001b[0m | \u001b[95m0.6859   \u001b[0m | \u001b[95m0.0008406\u001b[0m | \u001b[95m1.23e+03 \u001b[0m | \u001b[95m420.2    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.3875   \u001b[0m | \u001b[0m0.01562  \u001b[0m | \u001b[0m2.156e+03\u001b[0m | \u001b[0m430.7    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.3502   \u001b[0m | \u001b[0m0.0232   \u001b[0m | \u001b[0m836.6    \u001b[0m | \u001b[0m326.9    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.2316   \u001b[0m | \u001b[0m0.04557  \u001b[0m | \u001b[0m2.401e+03\u001b[0m | \u001b[0m487.7    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.3397   \u001b[0m | \u001b[0m0.0402   \u001b[0m | \u001b[0m1.422e+03\u001b[0m | \u001b[0m342.6    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.5205   \u001b[0m | \u001b[0m0.0115   \u001b[0m | \u001b[0m608.2    \u001b[0m | \u001b[0m381.5    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.2483   \u001b[0m | \u001b[0m0.0429   \u001b[0m | \u001b[0m3.143e+03\u001b[0m | \u001b[0m452.9    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.3427   \u001b[0m | \u001b[0m0.02992  \u001b[0m | \u001b[0m770.0    \u001b[0m | \u001b[0m263.7    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.3344   \u001b[0m | \u001b[0m0.04196  \u001b[0m | \u001b[0m1.745e+03\u001b[0m | \u001b[0m218.4    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.3648   \u001b[0m | \u001b[0m0.01541  \u001b[0m | \u001b[0m3.155e+03\u001b[0m | \u001b[0m405.9    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.3613   \u001b[0m | \u001b[0m0.01637  \u001b[0m | \u001b[0m2.172e+03\u001b[0m | \u001b[0m189.2    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.6553   \u001b[0m | \u001b[0m0.00215  \u001b[0m | \u001b[0m1.733e+03\u001b[0m | \u001b[0m102.1    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.5315   \u001b[0m | \u001b[0m0.005735 \u001b[0m | \u001b[0m2.433e+03\u001b[0m | \u001b[0m264.0    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.519    \u001b[0m | \u001b[0m0.006923 \u001b[0m | \u001b[0m1.807e+03\u001b[0m | \u001b[0m378.0    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.4515   \u001b[0m | \u001b[0m0.01091  \u001b[0m | \u001b[0m2.783e+03\u001b[0m | \u001b[0m314.2    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.6321   \u001b[0m | \u001b[0m0.00516  \u001b[0m | \u001b[0m583.7    \u001b[0m | \u001b[0m247.9    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.3475   \u001b[0m | \u001b[0m0.02185  \u001b[0m | \u001b[0m1.022e+03\u001b[0m | \u001b[0m130.3    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.6606   \u001b[0m | \u001b[0m0.002109 \u001b[0m | \u001b[0m775.5    \u001b[0m | \u001b[0m300.7    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.341    \u001b[0m | \u001b[0m0.02602  \u001b[0m | \u001b[0m3.057e+03\u001b[0m | \u001b[0m415.8    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.4854   \u001b[0m | \u001b[0m0.009905 \u001b[0m | \u001b[0m1.537e+03\u001b[0m | \u001b[0m452.0    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.3371   \u001b[0m | \u001b[0m0.03814  \u001b[0m | \u001b[0m1.905e+03\u001b[0m | \u001b[0m178.4    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.339    \u001b[0m | \u001b[0m0.04169  \u001b[0m | \u001b[0m798.6    \u001b[0m | \u001b[0m114.5    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.3416   \u001b[0m | \u001b[0m0.02474  \u001b[0m | \u001b[0m3.142e+03\u001b[0m | \u001b[0m115.6    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.4061   \u001b[0m | \u001b[0m0.01274  \u001b[0m | \u001b[0m3.117e+03\u001b[0m | \u001b[0m152.3    \u001b[0m |\n",
      "=============================================================\n",
      "CPU times: user 3h 17min 59s, sys: 1min 37s, total: 3h 19min 37s\n",
      "Wall time: 3h 21min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pbounds = {'n_estimators':(100,500),'max_samples':(400,len(data)//1.5),'ccp_alpha':(0,0.05)}\n",
    "n_iter = 40\n",
    "init_points = 5\n",
    "\n",
    "mfcc_optimal_parameters = optimize_RF(n_iter,init_points,pbounds,X_mfcc,y_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921af2a7-7ce4-41e9-99e6-9e651223c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./parameters/RF_mfcc.pckl','wb') as f:\n",
    "    pickle.dump(mfcc_optimal_parameters,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
